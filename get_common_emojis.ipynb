{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from emoji import UNICODE_EMOJI\n",
    "import nltk\n",
    "from utils import Indexer\n",
    "emoji_set = set(UNICODE_EMOJI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = text.replace(\"RT \", \"\")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    words = text.split(\" \")\n",
    "\n",
    "    for word in words:\n",
    "        if word.startswith(\"@\") or word.startswith(\"http\"):\n",
    "            text = text.replace(word, \"\")\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emojis_in_text(text, emoji_set):\n",
    "    char_set = set(text)\n",
    "    #print(char_set)\n",
    "    all_emojis = emoji_set & char_set\n",
    "    \n",
    "    return all_emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet):\n",
    "    cleaned_str = tweet['processed_text']\n",
    "    for emoji in tweet['emojis_in_text']:\n",
    "        cleaned_str = tweet['processed_text'].replace(emoji, \"\")\n",
    "        \n",
    "    return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading csv...\n",
      "done\n",
      "processing text\n",
      "processed 500000 tweets\n",
      "processed 1000000 tweets\n",
      "processed 1500000 tweets\n",
      "processed 2000000 tweets\n",
      "processed 2500000 tweets\n",
      "processed 3000000 tweets\n",
      "processed 3500000 tweets\n",
      "processed 4000000 tweets\n",
      "processed 4500000 tweets\n",
      "processed 5000000 tweets\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "with open('twitter_october.csv', 'r') as csv_file:\n",
    "    print(\"reading csv...\")\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    print(\"done\")\n",
    "    print(\"processing text\")\n",
    "    count = 0\n",
    "    for row in csv_reader:\n",
    "        if row['text'] == 'text':\n",
    "            continue\n",
    "        row['processed_text'] = process_text(row['text'])\n",
    "        row['emojis_in_text'] = get_emojis_in_text(row['processed_text'], emoji_set)\n",
    "        tweets.append(row)\n",
    "        count += 1\n",
    "        if count % 500000 == 0:\n",
    "            print(\"processed\", count, \"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5169003"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('created_at', 'Sat Oct 20 20:41:00 +0000 2018'),\n",
       "             ('id', '1053747937560420357'),\n",
       "             ('id_str', '1053747937560420357'),\n",
       "             ('text',\n",
       "              'RT @Anthonyybaby: Said Michael was from Texas 😂😂 https://t.co/Y5w9DM6Zv3'),\n",
       "             ('in_reply_to_status_id', ''),\n",
       "             ('user_id', '1546406606'),\n",
       "             ('user_id_str', '1546406606'),\n",
       "             ('user_screen_name', 'sothats_jada'),\n",
       "             ('processed_text', 'Said Michael was from Texas 😂😂'),\n",
       "             ('emojis_in_text', {'😂'})])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_labels(tweet, indexer):\n",
    "    \n",
    "    all_emojis = tweet['emojis_in_text']\n",
    "    emojis = {}\n",
    "    c = Counter()\n",
    "    for emoji in all_emojis:\n",
    "        indexer.add_and_get_index(emoji)\n",
    "        c[emoji] += tweet['processed_text'].count(emoji)\n",
    "    \n",
    "    #print(all_emojis)\n",
    "    #print(\"txt\", tweet['processed_text'])\n",
    "    max_score = max(c.values())\n",
    "    labels = [indexer.index_of(k) for k in c if c[k] == max_score]\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPoint():\n",
    "    def __init__(self, text, label):\n",
    "        self.text = text\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tweets, indexer, label_counter):\n",
    "    dataset = []\n",
    "    count = 0\n",
    "    for idx, tweet in enumerate(tweets):\n",
    "        #print(idx, tweet['processed_text'])\n",
    "        try:\n",
    "            labels = get_labels(tweet, indexer)\n",
    "        except:\n",
    "            continue\n",
    "        cleaned_text = clean_text(tweet)\n",
    "        for label in labels:\n",
    "            if indexer.get_object(label) is not '♀' and indexer.get_object(label) is not '♂':\n",
    "                datapoint = DataPoint(cleaned_text, label)\n",
    "                dataset.append(datapoint)\n",
    "                label_counter[indexer.get_object(label)] += 1\n",
    "                \n",
    "        count += 1\n",
    "        if count % 500000 == 0:\n",
    "            print(\"created\", count, \"datapoints\")\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created 500000 datapoints\n",
      "created 1000000 datapoints\n",
      "created 1500000 datapoints\n",
      "created 2000000 datapoints\n",
      "created 2500000 datapoints\n",
      "created 3000000 datapoints\n",
      "created 3500000 datapoints\n",
      "created 4000000 datapoints\n",
      "created 4500000 datapoints\n",
      "created 5000000 datapoints\n"
     ]
    }
   ],
   "source": [
    "indexer = Indexer()\n",
    "label_counter = Counter()\n",
    "dataset = create_dataset(tweets, indexer, label_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('😂', 617504), ('😭', 294681), ('❤', 280220), ('😍', 200159), ('🔥', 168289), ('🤣', 102484), ('💕', 101975), ('🙏', 97199), ('♀', 92854), ('✨', 87744), ('😩', 87401), ('💜', 86638), ('♂', 77390), ('😊', 74571), ('💀', 68176), ('🤷', 67752), ('👀', 66779), ('💙', 64025), ('🎃', 63107), ('🤔', 62330), ('😘', 62149), ('👏', 62025), ('🙌', 59498), ('🎉', 58619), ('💯', 58415), ('🤦', 55387), ('👍', 54447), ('👉', 54279), ('💖', 50452), ('🙄', 49965), ('😎', 49308), ('😁', 46164)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-88fb5c381444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "most_common = label_counter.most_common(32)\n",
    "print(most_common)\n",
    "\n",
    "print(most_common[8][0].decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['😂', '😭', '❤', '😍', '🔥', '🤣', '💕', '🙏', '✨', '😩', '💜', '😊', '💀', '🤷', '👀', '💙', '🎃', '🤔', '😘', '👏', '🙌', '🎉', '💯', '🤦', '👍', '👉', '💖', '🙄', '😎', '😁']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_emojis = []\n",
    "for idx, x in enumerate(most_common):\n",
    "    if idx == 8 or idx == 12:\n",
    "        continue\n",
    "    common_emojis.append(x[0])\n",
    "print(common_emojis)\n",
    "\n",
    "len(common_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
